{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 2&3\n",
    "#### Student Name: Krishnakanth Srikanth\n",
    "#### Student ID: s3959200\n",
    "\n",
    "Date: 01/10/2023\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "* CountVectorizer\n",
    "* TfidfVectorizer\n",
    "* genism\n",
    "* api\n",
    "* train_test_split\n",
    "* LogisticRegression\n",
    "* FreqDist\n",
    "* KFold\n",
    "* RegexpTokenizer\n",
    "* sent_tokenize\n",
    "* chain\n",
    "\n",
    "## Introduction\n",
    "In this task, feature representation of documents such as count vector, weighted and unweighted TF-IDF vector are to be calculated for job description. Using which Logistic Regression model is built, and the best vector representation is found out.\n",
    "Additionally, a question if adding more information like title to the description, affects the model accuracy or not is answered in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#pip install gensim\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Generating Feature Representations for Job Advertisement Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 different types of feature representation of documents are to be built in this task - count vector, two document embeddings (one TF-IDF weighted, and one unweighted version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Generating Count vector using Bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the vocab.txt file from task 1 to train BOW model\n",
    "# Empty list to append words\n",
    "vocab_list = []\n",
    "\n",
    "with open(\"vocab.txt\") as f:\n",
    "    vocab = f.readline()\n",
    "    while vocab:\n",
    "        end = vocab.find(\":\")\n",
    "        vocab_list.append(vocab[:end])\n",
    "        vocab = f.readline()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerating the job descriptions from task 1\n",
    "with open(\"job_desc.txt\", \"r\") as f:\n",
    "    job_desc = f.read().splitlines()\n",
    "job_desc = [desc.split(\" \") for desc in job_desc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Count vector generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vector generation\n",
    "joined_job_desc = [' '.join(desc) for desc in job_desc]\n",
    "cVectorizer = CountVectorizer(analyzer = \"word\", vocabulary = vocab_list)\n",
    "count_features = cVectorizer.fit_transform(joined_job_desc)\n",
    "count_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JD 1:\n",
      "accountant partqualified south east london manufacturing requirement accountant permanent modern offices south east london credit control purchase ledger daily collection debts phone letter email handling ledger accounts handling accounts negotiating payment terms cash reconciliation accounts adhoc administration duties person ideal previous credit control capacity possess exceptional customer communication part fully qualified accountant considered\n",
      "\n",
      "Count vector for JD 1: \n",
      "accountant:3 accounts:3 adhoc:1 administration:1 capacity:1 cash:1 collection:1 communication:1 considered:1 control:2 credit:2 customer:1 daily:1 debts:1 duties:1 east:2 email:1 exceptional:1 fully:1 handling:2 ideal:1 ledger:2 letter:1 london:2 manufacturing:1 modern:1 negotiating:1 offices:1 part:1 partqualified:1 payment:1 permanent:1 person:1 phone:1 possess:1 previous:1 purchase:1 qualified:1 reconciliation:1 requirement:1 south:2 terms:1 "
     ]
    }
   ],
   "source": [
    "# Check for count_features to see if the output is as expected\n",
    "print('JD 1:' + '\\n' + joined_job_desc[0] + '\\n\\n' + 'Count vector for JD 1: ')\n",
    "for word, value in zip(vocab_list, count_features.toarray()[0]): \n",
    "        if value > 0:\n",
    "            print(word+\":\"+str(value), end =' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the web_index from task1\n",
    "web_indices = []\n",
    "with open(\"web_indices.txt\", \"r\") as f:\n",
    "    # -1 to remove later spaces\n",
    "    idx = f.readline()[:-1]\n",
    "    while idx:\n",
    "        web_indices.append(idx)\n",
    "        idx = f.readline()[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving outputs\n",
    "Save the count vector representation as per spectification.\n",
    "- count_vectors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_count_vector(count_vector, filename, web_indices):\n",
    "    '''\n",
    "        This function is to store the count vectors to a new text file\n",
    "    '''\n",
    "    # total number of documents\n",
    "    num = count_vector.shape[0] \n",
    "    \n",
    "    # create a new txt file if it doesn't exists \n",
    "    cvector_file = open(filename, 'w') \n",
    "    \n",
    "    for i in range(0, num): \n",
    "        web_index = web_indices[i]\n",
    "        cvector_file.write(\"#\" + web_index + \",\")\n",
    "        \n",
    "        # for each word index that has non-zero entry in the count_vector\n",
    "        for j in count_vector[i].nonzero()[1]: \n",
    "            \n",
    "            # retrieve the value of the entry from count_vector\n",
    "            value = count_vector[i][0, j]\n",
    "            \n",
    "            if int(np.where(count_vector[i].nonzero()[1] == j)[0]) == len(count_vector[i].nonzero()[1])-1:\n",
    "                cvector_file.write(\"{}:{} \".format(j, value))\n",
    "                \n",
    "            else:\n",
    "                # write the entry to the file in the format of word_index:value\n",
    "                cvector_file.write(\"{}:{},\".format(j, value)) \n",
    "        \n",
    "        cvector_file.write('\\n') \n",
    "    \n",
    "    # close the file\n",
    "    cvector_file.close()\n",
    "    \n",
    "write_count_vector(count_features,\"count_vectors.txt\", web_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Models based on word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 5168)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate TF-IDF vectors :\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialised the the TfidfVectorizer\n",
    "tVectorizer = TfidfVectorizer(analyzer = \"word\", vocabulary = vocab_list)\n",
    "\n",
    "# generate the TF-IDF vector representation for all description\n",
    "tfidf_features = tVectorizer.fit_transform(joined_job_desc) \n",
    "\n",
    "tfidf_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_docVecs_weighted(wv,tk_txts,tfidf = []):\n",
    "    '''\n",
    "        Function to generate weighted vector representations for documents\n",
    "    '''\n",
    "    docs_vectors = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(tk_txts)):\n",
    "        tokens = list(set(tk_txts[i])) # Distinct words of document is collected using set()\n",
    "\n",
    "        temp = pd.DataFrame()\n",
    "        for w_ind in range(0, len(tokens)):\n",
    "            try:\n",
    "                word = tokens[w_ind]\n",
    "                word_vec = wv[word]\n",
    "                \n",
    "                if tfidf != []:\n",
    "                    word_weight = float(tfidf[i][word])\n",
    "                else:\n",
    "                    word_weight = 1\n",
    "                temp = temp.append(pd.Series(word_vec*word_weight), ignore_index = True)\n",
    "            except:\n",
    "                pass\n",
    "        doc_vector = temp.sum()\n",
    "        # append each document value to the final dataframe\n",
    "        docs_vectors = docs_vectors.append(doc_vector, ignore_index = True)\n",
    "    return docs_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to generate vector representation for documents - unweighted\n",
    "def gen_docVecs_unweighted(wv, tk_txts): \n",
    "    '''\n",
    "        Function to generate unweighted vector representations for documents\n",
    "    '''\n",
    "    # creating empty final dataframe\n",
    "    docs_vectors = pd.DataFrame()\n",
    "\n",
    "    for i in range(0, len(tk_txts)):\n",
    "        tokens = tk_txts[i]\n",
    "        \n",
    "        temp = pd.DataFrame() \n",
    "        \n",
    "        for w_ind in range(0, len(tokens)): \n",
    "            try:\n",
    "                word = tokens[w_ind]\n",
    "                \n",
    "                 # if word is present in embeddings then proceed\n",
    "                word_vec = wv[word]\n",
    "                \n",
    "                temp = temp.append(pd.Series(word_vec), ignore_index = True) \n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # sum across rows of each column\n",
    "        doc_vector = temp.sum() \n",
    "        \n",
    "        # append each document value to the final dataframe\n",
    "        docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) \n",
    "        \n",
    "    return docs_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vocIndex(voc_fname):\n",
    "    '''\n",
    "        This function reads the the vocabulary file, and create an w_index:word dictionary\n",
    "    '''\n",
    "    with open(voc_fname) as vocf: \n",
    "        voc_Ind = [l.split(':') for l in vocf.read().splitlines()] # each line is 'index,word'\n",
    "    return {int(vi[1]):vi[0] for vi in voc_Ind}\n",
    "\n",
    "# Generates the w_index:word dictionary\n",
    "voc_fname = 'vocab.txt' # path for the vocabulary\n",
    "voc_dict = gen_vocIndex(voc_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for the weights \n",
    "num = tfidf_features.shape[0] # the number of document\n",
    "tfidf_weights =[]\n",
    "\n",
    "for wt in range(0, num): \n",
    "    weight_dict = {}\n",
    "    for word, value in zip(vocab_list, tfidf_features.toarray()[wt]): \n",
    "        if value > 0:\n",
    "            weight_dict[word] = value\n",
    "    tfidf_weights.append(weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Embedding language model - GoogleNews300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec google news 300 api\n",
    "google_api = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the job descriptions to txt file\n",
    "with open(\"jd_task1.txt\", \"w\") as file:\n",
    "    for i in range(0, len(joined_job_desc)):\n",
    "        file.write(joined_job_desc[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization of description\n",
    "with open('jd_task1.txt') as file:\n",
    "    desc_text = file.read().splitlines() \n",
    "tokenized_description = [a.split(' ') for a in desc_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TF-IDF Unweighted representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Unweighted representation\n",
    "unweighted_idf = gen_docVecs_unweighted(google_api, pd.Series(tokenized_description))\n",
    "unweighted_idf.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TF-IDF Weighted representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Weighted representation\n",
    "weighted_idf = gen_docVecs_weighted(google_api, pd.Series(tokenized_description),tfidf_weights)\n",
    "weighted_idf.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Job Advertisement Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Q1: Language model comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=0, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "# KFold cross validation with 5 folds\n",
    "from sklearn.model_selection import KFold\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits= num_folds, random_state=0, shuffle = True)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_train,X_test,y_train, y_test,seed):\n",
    "    '''\n",
    "        Function to build Logistic Regression model on features created\n",
    "    '''\n",
    "    model = LogisticRegression(random_state=seed,max_iter = 1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "with open('./job_category_target.txt') as f: \n",
    "    target = f.readlines()\n",
    "target=[i.strip('\\n') for i in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>weighted_idf</th>\n",
       "      <th>unweighted_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.839744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.883871</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.851613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.890323</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.812903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.877419</td>\n",
       "      <td>0.832258</td>\n",
       "      <td>0.806452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.916129</td>\n",
       "      <td>0.858065</td>\n",
       "      <td>0.832258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count weighted_idf unweighted_idf\n",
       "0  0.852564     0.852564       0.839744\n",
       "1  0.883871     0.896774       0.851613\n",
       "2  0.890323     0.870968       0.812903\n",
       "3  0.877419     0.832258       0.806452\n",
       "4  0.916129     0.858065       0.832258"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "seed=0\n",
    "num_models = 2\n",
    "model_df = pd.DataFrame(columns = ['count', 'weighted_idf','unweighted_idf'], index=range(num_folds))\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(list(range(0,len(target)))):\n",
    "    y_train = [str(target[i]) for i in train_index]\n",
    "    y_test = [str(target[i]) for i in test_index]\n",
    "    \n",
    "    X_train_count, X_test_count = count_features[train_index], count_features[test_index]\n",
    "    model_df.loc[fold,'count'] = evaluate(count_features[train_index],count_features[test_index], y_train, y_test, seed)\n",
    "    \n",
    "    X_train_wt, X_test_wt = weighted_idf.iloc[train_index, :-1], weighted_idf.iloc[test_index, :-1]\n",
    "    model_df.loc[fold,'weighted_idf'] = evaluate(X_train_wt, X_test_wt, y_train, y_test, seed)\n",
    "    \n",
    "    X_train_uwt, X_test_uwt = unweighted_idf.iloc[train_index, :-1], unweighted_idf.iloc[test_index, :-1]\n",
    "    model_df.loc[fold,'unweighted_idf'] = evaluate(X_train_uwt, X_test_uwt, y_train, y_test, seed)\n",
    "    \n",
    "    fold +=1\n",
    "    \n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count             0.884061\n",
       "weighted_idf      0.862126\n",
       "unweighted_idf    0.828594\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "model_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, it is clear that __COUNT VECTOR FEATURE REPRESENTATION__ performs the best followed by __WEIGHTED TF-IDF FEATURE REPRESENTATION__ with the Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Q2: Does more information provide higher accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### With only Title of the job advertisement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the titles file created in task 1\n",
    "titles = []\n",
    "with open(\"job_titles.txt\", \"r\") as file:\n",
    "    title = file.readline()\n",
    "    while title:\n",
    "        titles.append(title[:-1])\n",
    "        title = file.readline() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the titles\n",
    "def tokenizeData(raw_data):\n",
    "    # cover all words to lowercase\n",
    "    nl_data = raw_data.lower()\n",
    "    \n",
    "    # segment into sentences\n",
    "    sentences = sent_tokenize(nl_data)\n",
    "    \n",
    "    # tokenize each sentence\n",
    "    pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "    tokenizer = RegexpTokenizer(pattern) \n",
    "    token_lists = [tokenizer.tokenize(sen) for sen in sentences]\n",
    "    \n",
    "    # merge them into a list of tokens\n",
    "    tokenised_data = list(chain.from_iterable(token_lists))\n",
    "    return tokenised_data\n",
    "\n",
    "tokenized_job_titles = [tokenizeData(title) for title in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words with length less than 2\n",
    "tk_job_titles_g2 = [[token for token in title if len(token) >= 2] for title in tokenized_job_titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords_list = f.read().splitlines()\n",
    "\n",
    "# remove stop words\n",
    "tk_job_titles_stp = [[word for word in job if word not in stopwords_list] for job in tk_job_titles_g2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words that appear only once by term frequency\n",
    "words = list(chain.from_iterable(tk_job_titles_stp))\n",
    "\n",
    "# compute term frequency for each unique word/type\n",
    "term_freq = FreqDist(words)\n",
    "lessFreqWords = set(term_freq.hapaxes())\n",
    "\n",
    "def removeLessFreqWords(words):\n",
    "    '''\n",
    "        This function is to remove the words that appear only once in document based on term frequency\n",
    "    '''\n",
    "    return [w for w in words if w not in lessFreqWords]\n",
    "\n",
    "tk_removeLessTermFreq = [removeLessFreqWords(word) for word in tk_job_titles_stp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the top 50 most frequent words by document frequency\n",
    "words_2 = list(chain.from_iterable([set(tk) for tk in tk_removeLessTermFreq]))\n",
    "\n",
    "# find words that appear most commonly across documents\n",
    "doc_freq = FreqDist(words_2)  \n",
    "doc_freq_sorted = sorted(list(doc_freq.most_common(50)))\n",
    "\n",
    "# Creating a list to append the top 50 words\n",
    "doc_freq_words = []\n",
    "for i,j in doc_freq_sorted:\n",
    "    doc_freq_words.append(i)\n",
    "\n",
    "def removeTop50(words):\n",
    "    '''\n",
    "        This function is to remove top 50 frequent words based on document frequency\n",
    "    '''\n",
    "    return [word for word in words if word not in doc_freq_words]\n",
    "\n",
    "tk_removeMostDocumentFreq = [removeTop50(words) for words in tk_removeLessTermFreq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_titles = [\" \".join(token) for token in tk_removeLessTermFreq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Count vector representation for title\n",
    "jobTitle = list(chain.from_iterable(tk_removeLessTermFreq))\n",
    "title_vocab = set(jobTitle)\n",
    "cVectorizerTitle = CountVectorizer(analyzer = \"word\", vocabulary = title_vocab)\n",
    "\n",
    "# fit the model on job descriptions\n",
    "count_features_title = cVectorizerTitle.fit_transform(tokenized_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating TFIDF vector for generating weighted and unweighted word embeddings of decriptions\n",
    "tVectorizerTitle = TfidfVectorizer(analyzer = \"word\", vocabulary = title_vocab)\n",
    "tfidf_features_title = tVectorizerTitle.fit_transform(tokenized_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the tokenized job titles for later use\n",
    "with open(\"tk_titles.txt\", \"w\") as file:\n",
    "    for i in range(0, len(tokenized_titles)):\n",
    "        file.write(tokenized_titles[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'tk_titles.txt'\n",
    "with open(filename) as f:\n",
    "    title = f.read().splitlines()\n",
    "tk_titles_list = [a.split(' ') for a in title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Unweighted representation\n",
    "unweighted_idf_title = gen_docVecs_unweighted(google_api, pd.Series(tk_titles_list))\n",
    "unweighted_idf_title.fillna(0.0, inplace = True) # Replacing missing values with 0\n",
    "unweighted_idf_title.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Weighted representation\n",
    "weighted_idf_title = gen_docVecs_weighted(google_api, pd.Series(tk_titles_list),tfidf_weights)\n",
    "weighted_idf_title.fillna(0.0, inplace = True) # Replacing missing values with 0\n",
    "weighted_idf_title.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8287937743190662"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with only title - Unweighted\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(unweighted_idf_title, target, \n",
    "                                                                                 list(range(0,len(target))),test_size=0.33, \n",
    "                                                                                 random_state=0)\n",
    "model = LogisticRegression(max_iter = 100, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7626459143968871"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with only title - Weighted\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(weighted_idf_title, target, \n",
    "                                                                                 list(range(0,len(target))),test_size=0.33, \n",
    "                                                                                 random_state=0)\n",
    "model = LogisticRegression(max_iter = 100, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### With only Description of the job advertisement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8560311284046692"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with only description - Unweighted\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(unweighted_idf, target, \n",
    "                                                                                 list(range(0,len(target))),test_size=0.33, \n",
    "                                                                                 random_state=0)\n",
    "model = LogisticRegression(max_iter = 100, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8599221789883269"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with only description - Weighted\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(weighted_idf, target, \n",
    "                                                                                 list(range(0,len(target))),test_size=0.33, \n",
    "                                                                                 random_state=0)\n",
    "model = LogisticRegression(max_iter = 100, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### With both the Title and Description of the job advertisement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate title and description of each job advertisement and add to a list\n",
    "title_desc = []\n",
    "for i in range(0,len(target)):\n",
    "    tit_des = \" \".join(tk_titles_list[i]) + \" \" + joined_job_desc[i]\n",
    "    title_desc.append(tit_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize title_desc\n",
    "tk_title_desc = [tokenizeData(job) for job in title_desc]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vector generation\n",
    "words = list(chain.from_iterable(tk_title_desc))\n",
    "tit_desc_vocab = sorted(list(set(words)))\n",
    "cVectorizer = CountVectorizer(analyzer = \"word\", vocabulary = tit_desc_vocab)\n",
    "count_features_titdesc = cVectorizer.fit_transform(title_desc)\n",
    "count_features_titdesc.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF generation\n",
    "tVectorizer = TfidfVectorizer(analyzer = \"word\", vocabulary = tit_desc_vocab) \n",
    "tfidf_features_titdesc = tVectorizer.fit_transform(title_desc) \n",
    "tfidf_features_titdesc.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tit_desc = {}\n",
    "for i in range(0, len(tit_desc_vocab)):\n",
    "    dict_tit_desc[i] = tit_desc_vocab[i]\n",
    "\n",
    "num = tfidf_features_titdesc.shape[0]\n",
    "tfidf_weights_titdesc =[]\n",
    "\n",
    "for i in range(0, num): \n",
    "    weight_dict = {}\n",
    "    for word, value in zip(tit_desc_vocab, tfidf_features_titdesc.toarray()[i]): \n",
    "        if value > 0:\n",
    "            weight_dict[word] = value\n",
    "    tfidf_weights_titdesc.append(weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Unweighted representation\n",
    "unweighted_idf_titdes = gen_docVecs_unweighted(google_api, pd.Series(title_desc))\n",
    "unweighted_idf_titdes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Weighted representation\n",
    "weighted_idf_titdes = gen_docVecs_weighted(google_api, pd.Series(title_desc), tfidf_weights_titdesc)\n",
    "weighted_idf_titdes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5953307392996109"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with title and description - Unweighted\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(unweighted_idf_titdes, target, \n",
    "                                                                                 list(range(0,len(target))),test_size=0.33, \n",
    "                                                                                 random_state=0)\n",
    "model = LogisticRegression(max_iter = 100, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model with title and description - Weighted\n",
    "# X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(weighted_idf_titdes, target, list(range(0,len(target))),test_size=0.33, random_state=0)\n",
    "# model = LogisticRegression(max_iter = 100, random_state=0)\n",
    "# model.fit(X_train, y_train)\n",
    "# model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above it is clear that adding information seems to lower the accuracy the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Challenging tasks. Learnt many new stuffs and had faced many errors during this task, which were stepping stones to complete the assignment. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
